{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import openai\n",
    "from llama_index.core.tools import BaseTool, FunctionTool\n",
    "from llama_index.tools.google import GmailToolSpec\n",
    "from llama_index.tools.google import GoogleCalendarToolSpec\n",
    "from llama_index.tools.google import GoogleSearchToolSpec\n",
    "# from llama_index.tools.wikipedia import WikipediaToolSpec\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "gmail_tools = GmailToolSpec().to_tool_list()\n",
    "gcal_tools = GoogleCalendarToolSpec().to_tool_list()\n",
    "gsearch_tools = GoogleSearchToolSpec(key=os.getenv('GOOGLE_SEARCH_API_KEY'), engine=\"engine\").to_tool_list()\n",
    "# wikipedia_tools = WikipediaToolSpec().to_tool_list(\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiple two integers and returns the result integer\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
    "\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers and returns the result integer\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "add_tool = FunctionTool.from_defaults(fn=add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_data\n",
      "load_data() -> List[llama_index.core.schema.Document]\n",
      "Load emails from the user's account.\n",
      "search_messages\n",
      "search_messages(query: str, max_results: Optional[int] = None)\n",
      "Searches email messages given a query string and the maximum number\n",
      "        of results requested by the user\n",
      "           Returns: List of relevant message objects up to the maximum number of results.\n",
      "\n",
      "        Args:\n",
      "            query[str]: The user's query\n",
      "            max_results (Optional[int]): The maximum number of search results\n",
      "            to return.\n",
      "        \n",
      "create_draft\n",
      "create_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None) -> str\n",
      "Create and insert a draft email.\n",
      "           Print the returned draft's message and id.\n",
      "           Returns: Draft object, including draft id and message meta data.\n",
      "\n",
      "        Args:\n",
      "            to (Optional[str]): The email addresses to send the message to\n",
      "            subject (Optional[str]): The subject for the event\n",
      "            message (Optional[str]): The message for the event\n",
      "        \n",
      "update_draft\n",
      "update_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None, draft_id: str = None) -> str\n",
      "Update a draft email.\n",
      "           Print the returned draft's message and id.\n",
      "           This function is required to be passed a draft_id that is obtained when creating messages\n",
      "           Returns: Draft object, including draft id and message meta data.\n",
      "\n",
      "        Args:\n",
      "            to (Optional[str]): The email addresses to send the message to\n",
      "            subject (Optional[str]): The subject for the event\n",
      "            message (Optional[str]): The message for the event\n",
      "            draft_id (str): the id of the draft to be updated\n",
      "        \n",
      "get_draft\n",
      "get_draft(draft_id: str = None) -> str\n",
      "Get a draft email.\n",
      "           Print the returned draft's message and id.\n",
      "           Returns: Draft object, including draft id and message meta data.\n",
      "\n",
      "        Args:\n",
      "            draft_id (str): the id of the draft to be updated\n",
      "        \n",
      "send_draft\n",
      "send_draft(draft_id: str = None) -> str\n",
      "Sends a draft email.\n",
      "           Print the returned draft's message and id.\n",
      "           Returns: Draft object, including draft id and message meta data.\n",
      "\n",
      "        Args:\n",
      "            draft_id (str): the id of the draft to be updated\n",
      "        \n",
      "load_data\n",
      "load_data(number_of_results: Optional[int] = 100, start_date: Union[str, datetime.date, NoneType] = None) -> List[llama_index.core.schema.Document]\n",
      "Load data from user's calendar.\n",
      "\n",
      "        Args:\n",
      "            number_of_results (Optional[int]): the number of events to return. Defaults to 100.\n",
      "            start_date (Optional[Union[str, datetime.date]]): the start date to return events from in date isoformat. Defaults to today.\n",
      "        \n",
      "create_event\n",
      "create_event(title: Optional[str] = None, description: Optional[str] = None, location: Optional[str] = None, start_datetime: Union[str, datetime.datetime, NoneType] = None, end_datetime: Union[str, datetime.datetime, NoneType] = None, attendees: Optional[List[str]] = None) -> str\n",
      "\n",
      "            Create an event on the users calendar.\n",
      "\n",
      "        Args:\n",
      "            title (Optional[str]): The title for the event\n",
      "            description (Optional[str]): The description for the event\n",
      "            location (Optional[str]): The location for the event\n",
      "            start_datetime Optional[Union[str, datetime.datetime]]: The start datetime for the event\n",
      "            end_datetime Optional[Union[str, datetime.datetime]]: The end datetime for the event\n",
      "            attendees Optional[List[str]]: A list of email address to invite to the event\n",
      "        \n",
      "get_date\n",
      "get_date()\n",
      "\n",
      "        A function to return todays date. Call this before any other functions if you are unaware of the date.\n",
      "        \n",
      "google_search\n",
      "google_search(query: str)\n",
      "\n",
      "        Make a query to the Google search engine to receive a list of results.\n",
      "\n",
      "        Args:\n",
      "            query (str): The query to be passed to Google search.\n",
      "            num (int, optional): The number of search results to return. Defaults to None.\n",
      "\n",
      "        Raises:\n",
      "            ValueError: If the 'num' is not an integer between 1 and 10.\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "for tool in [*gmail_tools, *gcal_tools, *gsearch_tools]:\n",
    "    print(tool.metadata.name)\n",
    "    print(tool.metadata.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.tools.tool_spec.load_and_search.base import LoadAndSearchToolSpec\n",
    "from llama_index.core.tools.tool_spec.load_and_search import ( LoadAndSearchToolSpec,)\n",
    "\n",
    "print(\"Wrapping \" + gsearch_tools[0].metadata.name)\n",
    "gsearch_load_and_search_tools = LoadAndSearchToolSpec.from_defaults( gsearch_tools[0],).to_tool_list()\n",
    "\n",
    "print(\"Wrapping gmail \" + gmail_tools[0].metadata.name)\n",
    "gmail_load_and_search_tools = LoadAndSearchToolSpec.from_defaults( gmail_tools[0],).to_tool_list()\n",
    "\n",
    "print(\"Wrapping google calendar \" + gcal_tools[0].metadata.name)\n",
    "gcal_load_and_search_tools = LoadAndSearchToolSpec.from_defaults( gcal_tools[0],).to_tool_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tools = [\n",
    "    # *gsearch_load_and_search_tools,\n",
    "    # *gmail_load_and_search_tools,\n",
    "    # *gcal_load_and_search_tools,\n",
    "    # *gcal_tools[1::],\n",
    "    # *gmail_tools[1::],\n",
    "    # *gsearch_tools[1::],\n",
    "    multiply_tool,\n",
    "    add_tool,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 30] Read-only file system: '/nix/store/jbc1v12hh5lnfa4030zyp99hxqjc5pzx-python3.11-llama-index-core-0.11.23/lib/python3.11/site-packages/llama_index/core/_static/tiktoken_cache/9b5ad71b2ce5302211f9c61530b329a4922fc6a4.62e4a395-22ce-40b4-bbf2-6bbe29d8bab8.tmp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m llm \u001b[38;5;241m=\u001b[39m OpenAI(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo-0613\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAIAgent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_tools\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nix/store/202rz70n01ryfdhmqkhqphsd1a22kwdf-devenv-profile/lib/python3.11/site-packages/llama_index/agent/openai/base.py:116\u001b[0m, in \u001b[0;36mOpenAIAgent.from_tools\u001b[0;34m(cls, tools, tool_retriever, llm, chat_history, memory, memory_cls, verbose, max_function_calls, default_tool_choice, callback_manager, system_prompt, prefix_messages, tool_call_parser, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callback_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     llm\u001b[38;5;241m.\u001b[39mcallback_manager \u001b[38;5;241m=\u001b[39m callback_manager\n\u001b[0;32m--> 116\u001b[0m memory \u001b[38;5;241m=\u001b[39m memory \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mmemory_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_defaults\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_history\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m llm\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mis_function_calling_model:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mllm\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not support function calling API. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m     )\n",
      "File \u001b[0;32m/nix/store/jbc1v12hh5lnfa4030zyp99hxqjc5pzx-python3.11-llama-index-core-0.11.23/lib/python3.11/site-packages/llama_index/core/memory/chat_memory_buffer.py:74\u001b[0m, in \u001b[0;36mChatMemoryBuffer.from_defaults\u001b[0;34m(cls, chat_history, llm, chat_store, chat_store_key, token_limit, tokenizer_fn, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m     chat_store \u001b[38;5;241m=\u001b[39m chat_store \u001b[38;5;129;01mor\u001b[39;00m SimpleChatStore()\n\u001b[1;32m     70\u001b[0m     chat_store\u001b[38;5;241m.\u001b[39mset_messages(chat_store_key, chat_history)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m     73\u001b[0m     token_limit\u001b[38;5;241m=\u001b[39mtoken_limit,\n\u001b[0;32m---> 74\u001b[0m     tokenizer_fn\u001b[38;5;241m=\u001b[39mtokenizer_fn \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mget_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     75\u001b[0m     chat_store\u001b[38;5;241m=\u001b[39mchat_store \u001b[38;5;129;01mor\u001b[39;00m SimpleChatStore(),\n\u001b[1;32m     76\u001b[0m     chat_store_key\u001b[38;5;241m=\u001b[39mchat_store_key,\n\u001b[1;32m     77\u001b[0m )\n",
      "File \u001b[0;32m/nix/store/jbc1v12hh5lnfa4030zyp99hxqjc5pzx-python3.11-llama-index-core-0.11.23/lib/python3.11/site-packages/llama_index/core/utils.py:129\u001b[0m, in \u001b[0;36mget_tokenizer\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m     should_revert \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    124\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTIKTOKEN_CACHE_DIR\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m    125\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;18m__file__\u001b[39m)),\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_static/tiktoken_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    127\u001b[0m     )\n\u001b[0;32m--> 129\u001b[0m enc \u001b[38;5;241m=\u001b[39m \u001b[43mtiktoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding_for_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m partial(enc\u001b[38;5;241m.\u001b[39mencode, allowed_special\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    131\u001b[0m set_global_tokenizer(tokenizer)\n",
      "File \u001b[0;32m/nix/store/202rz70n01ryfdhmqkhqphsd1a22kwdf-devenv-profile/lib/python3.11/site-packages/tiktoken/model.py:97\u001b[0m, in \u001b[0;36mencoding_for_model\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencoding_for_model\u001b[39m(model_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Encoding:\n\u001b[1;32m     93\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the encoding used by a model.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m    Raises a KeyError if the model name is not recognised.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding_name_for_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nix/store/202rz70n01ryfdhmqkhqphsd1a22kwdf-devenv-profile/lib/python3.11/site-packages/tiktoken/registry.py:73\u001b[0m, in \u001b[0;36mget_encoding\u001b[0;34m(encoding_name)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown encoding \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mencoding_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Plugins found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_available_plugin_modules()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m     )\n\u001b[1;32m     72\u001b[0m constructor \u001b[38;5;241m=\u001b[39m ENCODING_CONSTRUCTORS[encoding_name]\n\u001b[0;32m---> 73\u001b[0m enc \u001b[38;5;241m=\u001b[39m Encoding(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mconstructor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     74\u001b[0m ENCODINGS[encoding_name] \u001b[38;5;241m=\u001b[39m enc\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m enc\n",
      "File \u001b[0;32m/nix/store/202rz70n01ryfdhmqkhqphsd1a22kwdf-devenv-profile/lib/python3.11/site-packages/tiktoken_ext/openai_public.py:64\u001b[0m, in \u001b[0;36mcl100k_base\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcl100k_base\u001b[39m():\n\u001b[0;32m---> 64\u001b[0m     mergeable_ranks \u001b[38;5;241m=\u001b[39m \u001b[43mload_tiktoken_bpe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     special_tokens \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     68\u001b[0m         ENDOFTEXT: \u001b[38;5;241m100257\u001b[39m,\n\u001b[1;32m     69\u001b[0m         FIM_PREFIX: \u001b[38;5;241m100258\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m         ENDOFPROMPT: \u001b[38;5;241m100276\u001b[39m,\n\u001b[1;32m     73\u001b[0m     }\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcl100k_base\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpat_str\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m(?i:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mve|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mll|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md)|[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;132;01m{L}\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;132;01m{N}\u001b[39;00m\u001b[38;5;124m]?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;132;01m{L}\u001b[39;00m\u001b[38;5;124m+|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;132;01m{N}\u001b[39;00m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m1,3}| ?[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;132;01m{L}\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;132;01m{N}\u001b[39;00m\u001b[38;5;124m]+[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mn]*|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mn]+|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+(?!\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS)|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m,\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmergeable_ranks\u001b[39m\u001b[38;5;124m\"\u001b[39m: mergeable_ranks,\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecial_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: special_tokens,\n\u001b[1;32m     79\u001b[0m     }\n",
      "File \u001b[0;32m/nix/store/202rz70n01ryfdhmqkhqphsd1a22kwdf-devenv-profile/lib/python3.11/site-packages/tiktoken/load.py:116\u001b[0m, in \u001b[0;36mload_tiktoken_bpe\u001b[0;34m(tiktoken_bpe_file)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_tiktoken_bpe\u001b[39m(tiktoken_bpe_file: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# NB: do not add caching to this function\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m     contents \u001b[38;5;241m=\u001b[39m \u001b[43mread_file_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiktoken_bpe_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    118\u001b[0m         base64\u001b[38;5;241m.\u001b[39mb64decode(token): \u001b[38;5;28mint\u001b[39m(rank)\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m token, rank \u001b[38;5;129;01min\u001b[39;00m (line\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m contents\u001b[38;5;241m.\u001b[39msplitlines() \u001b[38;5;28;01mif\u001b[39;00m line)\n\u001b[1;32m    120\u001b[0m     }\n",
      "File \u001b[0;32m/nix/store/202rz70n01ryfdhmqkhqphsd1a22kwdf-devenv-profile/lib/python3.11/site-packages/tiktoken/load.py:52\u001b[0m, in \u001b[0;36mread_file_cached\u001b[0;34m(blobpath)\u001b[0m\n\u001b[1;32m     50\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(cache_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     51\u001b[0m tmp_filename \u001b[38;5;241m=\u001b[39m cache_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(uuid\u001b[38;5;241m.\u001b[39muuid4()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.tmp\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(tmp_filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     53\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(contents)\n\u001b[1;32m     54\u001b[0m os\u001b[38;5;241m.\u001b[39mrename(tmp_filename, cache_path)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: '/nix/store/jbc1v12hh5lnfa4030zyp99hxqjc5pzx-python3.11-llama-index-core-0.11.23/lib/python3.11/site-packages/llama_index/core/_static/tiktoken_cache/9b5ad71b2ce5302211f9c61530b329a4922fc6a4.62e4a395-22ce-40b4-bbf2-6bbe29d8bab8.tmp'"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n",
    "agent = OpenAIAgent.from_tools(tools=all_tools, llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.chat(\n",
    "  # \"search google and find the email address for a dentist in toronto near bloor and dufferin\"\n",
    "  \"search google and find the phone number for a dentist in ridgewood near east ridgewood avenue\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.chat(\"What does google return for that search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.chat(\"write an email to send to john@johnrizzo.net asking if we can meet tomorrow at 2pm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.chat(\"send an email to nonsleepr@gmail.com with the subject let's meet on Friday and the body this is the first email I've Ada has sent on my behalf.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.chat(\"What do I have on my calendar for the day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.chat(\"How many emails have I received today\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
