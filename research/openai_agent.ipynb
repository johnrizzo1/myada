{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from typing import Sequence, List\n",
    "\n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.tools import BaseTool, FunctionTool\n",
    "from openai.types.chat import ChatCompletionMessageToolCall\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiple two integers and returns the result integer\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
    "\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers and returns the result integer\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "add_tool = FunctionTool.from_defaults(fn=add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourOpenAIAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tools: Sequence[BaseTool] = [],\n",
    "        llm: OpenAI = OpenAI(temperature=0, model=\"gpt-3.5-turbo\"),\n",
    "        chat_history: List[ChatMessage] = [],\n",
    "    ) -> None:\n",
    "        self._llm = llm\n",
    "        self._tools = {tool.metadata.name: tool for tool in tools}\n",
    "        self._chat_history = chat_history\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self._chat_history = []\n",
    "\n",
    "    def chat(self, message: str) -> str:\n",
    "        chat_history = self._chat_history\n",
    "        chat_history.append(ChatMessage(role=\"user\", content=message))\n",
    "        tools = [\n",
    "            tool.metadata.to_openai_tool() for _, tool in self._tools.items()\n",
    "        ]\n",
    "\n",
    "        ai_message = self._llm.chat(chat_history, tools=tools).message\n",
    "        additional_kwargs = ai_message.additional_kwargs\n",
    "        chat_history.append(ai_message)\n",
    "\n",
    "        tool_calls = additional_kwargs.get(\"tool_calls\", None)\n",
    "        # parallel function calling is now supported\n",
    "        if tool_calls is not None:\n",
    "            for tool_call in tool_calls:\n",
    "                function_message = self._call_function(tool_call)\n",
    "                chat_history.append(function_message)\n",
    "                ai_message = self._llm.chat(chat_history).message\n",
    "                chat_history.append(ai_message)\n",
    "\n",
    "        return ai_message.content\n",
    "\n",
    "    def _call_function(\n",
    "        self, tool_call: ChatCompletionMessageToolCall\n",
    "    ) -> ChatMessage:\n",
    "        id_ = tool_call.id\n",
    "        function_call = tool_call.function\n",
    "        tool = self._tools[function_call.name]\n",
    "        output = tool(**json.loads(function_call.arguments))\n",
    "        return ChatMessage(\n",
    "            name=function_call.name,\n",
    "            content=str(output),\n",
    "            role=\"tool\",\n",
    "            additional_kwargs={\n",
    "                \"tool_call_id\": id_,\n",
    "                \"name\": function_call.name,\n",
    "            },\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "# agent = OpenAIAgent.from_tools([multiply_tool, add_tool], llm=llm, verbose=True)\n",
    "agent = YourOpenAIAgent(tools=[multiply_tool, add_tool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.chat(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The product of 2123 and 215123 is 456,706,129.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.chat(\"What is 2123 * 215123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
